{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tU-25xISaruQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "zHluMPJGhfQ7"
   },
   "source": [
    "# **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uw3nUK7Zhwrv"
   },
   "source": [
    "Pre Scriptum: Т.к. я не успела сделать первый кейс, предлагаю вам посмотреть на решение похоже задачи, реализованное мной для отбора на Уральскую Проектную Смену в октябре 2019 года. Спасибо.\n",
    "\n",
    "**Необходимые библиотеки:**\n",
    "* skitilearn\n",
    "* xgboost\n",
    "* codecs\n",
    "* eli5\n",
    "* textbolb\n",
    "* numpy\n",
    "* pandas\n",
    "* ntlk\n",
    "* re\n",
    "* codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MPUkDS7i6g7"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost \n",
    "!pip install eli5\n",
    "!pip install textblob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_unGYNJjAul"
   },
   "source": [
    "# Шаг 1: Работа с данными\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vJxbDerjVdW"
   },
   "source": [
    "Загрузим тренировочные и тестовые выборки из датасета yelp, а также набор **'stopwords'** - слова в языке, не несущие смысловую (только грамматическую) нагрузку. \n",
    "\n",
    "*Например: this, is, therefore, etc.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hXknboEjSlr"
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "train_input_labels = codecs.open(r'/content/yelp/sentiment.train.labels', 'r')\n",
    "train_input_text = codecs.open(r'/content/yelp/sentiment.train.text', 'r')\n",
    "\n",
    "# test data\n",
    "test_input_labels = codecs.open(r'/content/yelp/sentiment.test.labels', 'r')\n",
    "test_input_text = codecs.open(r'/content/yelp/sentiment.test.text', 'r')\n",
    "\n",
    "# stop words corpus\n",
    "stop = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7SIFhiXkRgC"
   },
   "source": [
    "Так как изначально данные представленны в виде файла без определённого формата, требуется представить их в виде массива. \n",
    "\n",
    "Для оптимизации одновремено с этим будем производить работу с данными, а именно:\n",
    "1.   Приведём все слова в lower case\n",
    "2.   Избавимся от пунктуации\n",
    "3.   Лемматизируем, т.е. приведём к словарной (по возможности - начальной) форме каждое слово\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjxPzjzGlUln"
   },
   "outputs": [],
   "source": [
    "# simple function to clean and cast data to list\n",
    "def get_list(input):\n",
    "    output = []\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "    for sentence in input:\n",
    "        sentence = sentence.lower()  \n",
    "        sentence.replace(\"\\n\", \"\")              \n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)     \n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)      \n",
    "    \n",
    "        words = [Word(word).lemmatize() for word in sentence.split()]\n",
    "        output.append(\" \".join(words))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# init train and test datasets\n",
    "train_x = get_list(train_input_text)\n",
    "train_y = get_list(train_input_labels)\n",
    "\n",
    "test_x = get_list(test_input_text)\n",
    "test_y = get_list(test_input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKKsX260likH"
   },
   "source": [
    "# Шаг 2: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBn5KBJll1bt"
   },
   "source": [
    "Следующий шаг - создание представления данных. \n",
    "\n",
    "Для решения поставленной задачи будем использовать **\"Мешок слов\"** как достаточно высокоуровневый метод представления данных. \n",
    "\n",
    "В этом случае предложение будет представлено как вектор длины **n**, где **n** - количество уникальных слов во всей выборке (длина условного словаря).\n",
    "Каждое слово будет представлено индексом - числом вхождений его в предложение.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqBPHL7xocnJ"
   },
   "outputs": [],
   "source": [
    "# feature engineering: count vectors\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "train_x_count =  count_vect.transform(train_x)\n",
    "test_x_count =  count_vect.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqPdomkFogvu"
   },
   "source": [
    "# Шаг 3: Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzIptMIgocA0"
   },
   "source": [
    "Наконец, приступаем к классификации. \n",
    "\n",
    "Самые популярные методы решения задачи о бинарной классификации:\n",
    "* Логистическая регрессия\n",
    "* Наивный Байесовский алгоритм\n",
    "* Дерево решений и случайный лес\n",
    "* Бустинг\n",
    "* Метод опорных векторов\n",
    "* Метод К-ближайших соседей\n",
    "\n",
    "Последние два метода слишком неэффективны, в то же время мы имеем достаточно большую тренировочную выборку, поэтому опустим их.\n",
    "\n",
    "Итак, произведём обучение четырёх названных нами моделей. Вынос в отдельный метод сделан для устранения повторяющегося кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSPEPy_rpsuT"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, train_y, feature_vector_test, test_y):\n",
    "    classifier.fit(feature_vector_train, train_y)\n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "\n",
    "    models.append(type(classifier).__name__)\n",
    "    res = get_metrics(predictions, test_y)\n",
    "    accuracy.append(res[0])\n",
    "    precision.append(res[1])\n",
    "    recall.append(res[2])\n",
    "    f1.append(res[3])\n",
    "\n",
    "\n",
    "# init metrics datasets\n",
    "accuracy, precision, recall, f1 = [], [], [], []\n",
    "models, labels = [], ['Negative', 'Positive']\n",
    "\n",
    "# init models\n",
    "LG_model = LogisticRegression(class_weight='balanced', solver='lbfgs', multi_class='multinomial', random_state=42,  n_jobs = -1)\n",
    "XGB_model = XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=100, nthread=50)\n",
    "RF_model = RandomForestClassifier(n_estimators=100, max_depth=42, random_state=42)\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "# train each model\n",
    "train_model(LG_model, train_x_count, train_y, test_x_count, test_y)\n",
    "train_model(NB_model, train_x_count, train_y, test_x_count, test_y)\n",
    "train_model(XGB_model, train_x_count.tocsc(), train_y, test_x_count.tocsc(), test_y)\n",
    "train_model(RF_model, train_x_count, train_y, test_x_count, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuiQN7RKp2Tp"
   },
   "source": [
    "После обучения каждой модели будем считать следующие метрики:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* F1\n",
    "* ReCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moFkK4fRi0PM"
   },
   "outputs": [],
   "source": [
    "def get_metrics(y_predicted, y_test):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None, average='weighted')  \n",
    "    \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XABIitNwqPjn"
   },
   "source": [
    "И выведем полученные данные в виде одной таблицы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNobs828t1c0"
   },
   "outputs": [],
   "source": [
    "# show results\n",
    "df_results = pd.DataFrame({'Model': models, 'Accuracy': accuracy, 'Precision': precision, 'ReCall': recall, 'F1': f1})\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjswXnqguAlW"
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1p0VQZxzKTNRgkUVrafJ8gEfHy9a3hTBK' />\n",
    "<figcaption></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2Qeuq3AtKEF"
   },
   "source": [
    "# Шаг 4: Интрепретация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVk0X4MntGc5"
   },
   "source": [
    "По итогам обучения самую высокую точность показала модель **Логистическая Регрессия - 0.96**. Её и будем интерпретировать. \n",
    "\n",
    "Для этого используем библиотеку eli5. Метод show_weights() наглядно представляет, какие слова оказали наибольшее влияние при классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmwFCqENseri"
   },
   "outputs": [],
   "source": [
    "# interpret the best model\n",
    "eli5.show_weights(LG_model, vec=count_vect, target_names = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6tTOYm8uLAo"
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1VXNQI_4LZqtBluydY3WL3dHmfy8KMogc' />\n",
    "<figcaption></figcaption></center>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "case3a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
